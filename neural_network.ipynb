{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlqFeZRKzsWV",
        "outputId": "985f106b-c801-4861-ed9a-f42c81385b51"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Flatten\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, AveragePooling2D\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the EMNIST dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m (ds_train, ds_test), ds_info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memnist/letters\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m )\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, AveragePooling2D\n",
        "\n",
        "\n",
        "# Load the EMNIST dataset\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'emnist/letters',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(128)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=473, kernel_size= (3, 3), activation=tf.nn.relu, input_shape=(28,28,1)),\n",
        "  tf.keras.layers.AveragePooling2D((2, 2)),\n",
        "  tf.keras.layers.Dropout(rate=0.15),\n",
        "  tf.keras.layers.Conv2D(filters=238, kernel_size= (3, 3), padding='valid', activation=tf.nn.leaky_relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.20),\n",
        "  tf.keras.layers.Conv2D(filters=133, kernel_size= (3, 3), activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.10),\n",
        "  tf.keras.layers.Conv2D(filters=387, kernel_size= (3, 3), activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.10),\n",
        "  tf.keras.layers.Conv2D(filters=187, kernel_size= (5, 5), activation=tf.nn.elu),\n",
        "  tf.keras.layers.Dropout(rate=0.50),\n",
        "  tf.keras.layers.Dense(313, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.20),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(252, activation=tf.nn.elu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(rate=0.20),\n",
        "  tf.keras.layers.Dense(37, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "# model.compile(\n",
        "#     loss='sparse_categorical_crossentropy',\n",
        "#     optimizer=tf.keras.optimizers.RMSprop(),\n",
        "#     metrics=['accuracy'],\n",
        "# )\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=10,\n",
        "    validation_data=ds_test\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(ds_test)\n",
        "print(f'Test loss: {test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
